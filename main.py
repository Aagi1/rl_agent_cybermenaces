"""
Agent Q-Learning pour la DÃ©tection et RÃ©ponse aux Cyberattaques
BasÃ© sur le paper: "Q-Learning Approach Applied to Network Security"
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.feature_selection import mutual_info_classif
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score
import random
import warnings
warnings.filterwarnings('ignore')

# ============================================
# PARTIE 1 : CONFIGURATION DE L'ENVIRONNEMENT
# ============================================

class CyberSecurityEnvironment:
    """
    Environnement simulÃ© pour l'entraÃ®nement de l'agent Q-learning
    """
    def __init__(self):
        # DÃ©finition des Ã©tats
        self.states = ['Normal', 'DoS', 'Probe', 'R2L', 'U2R']
        self.n_states = len(self.states)
        
        # DÃ©finition des actions
        self.actions = ['Allow', 'Report', 'Return', 'Block']
        self.n_actions = len(self.actions)
        
        # Matrice de rÃ©compenses (Ã©tats Ã— actions)
        self.rewards = np.array([
            [5, 0, 1, -1],    # Normal: Allow=5, Report=0, Return=1, Block=-1
            [-5, 3, 0, 10],   # DoS: Allow=-5, Report=3, Return=0, Block=10
            [0, 1, 2, 5],     # Probe: Allow=0, Report=1, Return=2, Block=5
            [-2, 1, 4, 6],    # R2L: Allow=-2, Report=1, Return=4, Block=6
            [-3, 0, 3, 8]     # U2R: Allow=-3, Report=0, Return=3, Block=8
        ])
        
        # Matrice de transition (sera estimÃ©e ou simulÃ©e)
        self.transition_probs = self._initialize_transition_probabilities()
        
    def _initialize_transition_probabilities(self):
        """
        Initialise les probabilitÃ©s de transition pour chaque action
        Format: {action: matrix (5Ã—5)} oÃ¹ matrix[i,j] = P(j|i,action)
        """
        transitions = {}
        
        # Action 0: Allow (probabilitÃ©s si on autorise le trafic)
        transitions[0] = np.array([
            [0.90, 0.03, 0.03, 0.02, 0.02],  # Normal â†’ ...
            [0.10, 0.70, 0.10, 0.05, 0.05],  # DoS â†’ ...
            [0.60, 0.10, 0.20, 0.05, 0.05],  # Probe â†’ ...
            [0.50, 0.10, 0.10, 0.20, 0.10],  # R2L â†’ ...
            [0.40, 0.10, 0.10, 0.10, 0.30]   # U2R â†’ ...
        ])
        
        # Action 1: Report (signaler Ã  l'admin)
        transitions[1] = np.array([
            [0.85, 0.05, 0.05, 0.03, 0.02],
            [0.40, 0.40, 0.10, 0.05, 0.05],
            [0.50, 0.10, 0.25, 0.10, 0.05],
            [0.45, 0.10, 0.10, 0.25, 0.10],
            [0.40, 0.10, 0.10, 0.10, 0.30]
        ])
        
        # Action 2: Return (renvoyer pour reclassification)
        transitions[2] = np.array([
            [0.80, 0.05, 0.05, 0.05, 0.05],
            [0.30, 0.50, 0.10, 0.05, 0.05],
            [0.40, 0.10, 0.35, 0.10, 0.05],
            [0.35, 0.10, 0.10, 0.35, 0.10],
            [0.30, 0.10, 0.10, 0.10, 0.40]
        ])
        
        # Action 3: Block (bloquer le trafic)
        transitions[3] = np.array([
            [0.70, 0.10, 0.10, 0.05, 0.05],  # Si on bloque Normal â†’ risque de rester bloquÃ©
            [0.85, 0.08, 0.03, 0.02, 0.02],  # Si on bloque DoS â†’ retour Ã  Normal
            [0.80, 0.05, 0.10, 0.03, 0.02],  # Si on bloque Probe â†’ retour Ã  Normal
            [0.80, 0.05, 0.05, 0.07, 0.03],  # Si on bloque R2L â†’ retour Ã  Normal
            [0.85, 0.05, 0.03, 0.02, 0.05]   # Si on bloque U2R â†’ retour Ã  Normal
        ])
        
        return transitions
    
    def reset(self):
        """RÃ©initialise l'environnement Ã  un Ã©tat alÃ©atoire"""
        return random.randint(0, self.n_states - 1)
    
    def step(self, state, action):
        """
        ExÃ©cute une action dans un Ã©tat donnÃ©
        Retourne: (next_state, reward, done)
        """
        # Obtenir la rÃ©compense
        reward = self.rewards[state, action]
        
        # Transition vers le prochain Ã©tat basÃ©e sur les probabilitÃ©s
        transition_prob = self.transition_probs[action][state]
        next_state = np.random.choice(self.n_states, p=transition_prob)
        
        # Episode terminÃ© alÃ©atoirement (10% de chance)
        done = random.random() < 0.1
        
        return next_state, reward, done


# ============================================
# PARTIE 2 : AGENT Q-LEARNING
# ============================================

class QLearningAgent:
    """
    Agent Q-Learning pour la rÃ©ponse aux intrusions
    """
    def __init__(self, n_states, n_actions, learning_rate=0.2, 
                 discount_factor=0.1, epsilon=0.9, epsilon_decay=0.05, 
                 epsilon_min=0.01):
        
        self.n_states = n_states
        self.n_actions = n_actions
        
        # HyperparamÃ¨tres
        self.alpha = learning_rate
        self.gamma = discount_factor
        self.epsilon = epsilon
        self.epsilon_decay = epsilon_decay
        self.epsilon_min = epsilon_min
        
        # Q-table (actions Ã— Ã©tats)
        self.Q = np.zeros((n_actions, n_states))
        
        # Historique
        self.episode_rewards = []
        
    def select_action(self, state):
        """
        StratÃ©gie Îµ-greedy pour sÃ©lectionner une action
        """
        if random.random() < self.epsilon:
            # Exploration: action alÃ©atoire
            return random.randint(0, self.n_actions - 1)
        else:
            # Exploitation: meilleure action connue
            return np.argmax(self.Q[:, state])
    
    def update_q_value(self, state, action, reward, next_state):
        """
        Mise Ã  jour de la Q-table selon l'Ã©quation de Bellman
        Q'(s,a) = (1-Î±)Q(s,a) + Î±[r + Î³Â·max(Q(s',a'))]
        """
        current_q = self.Q[action, state]
        max_future_q = np.max(self.Q[:, next_state])
        
        new_q = (1 - self.alpha) * current_q + self.alpha * (reward + self.gamma * max_future_q)
        
        self.Q[action, state] = new_q
    
    def decay_epsilon(self):
        """RÃ©duction progressive de l'exploration"""
        self.epsilon = max(self.epsilon * (1 - self.epsilon_decay), self.epsilon_min)
    
    def get_optimal_policy(self, states):
        """
        Extrait la politique optimale de la Q-table
        """
        policy = {}
        actions = ['Allow', 'Report', 'Return', 'Block']
        
        for state_idx, state_name in enumerate(states):
            best_action_idx = np.argmax(self.Q[:, state_idx])
            policy[state_name] = actions[best_action_idx]
        
        return policy


# ============================================
# PARTIE 3 : ENTRAÃŽNEMENT
# ============================================

def train_agent(env, agent, num_episodes=300, steps_per_episode=50):
    """
    EntraÃ®ne l'agent Q-learning dans l'environnement simulÃ©
    """
    print("="*60)
    print("DÃ‰BUT DE L'ENTRAÃŽNEMENT")
    print("="*60)
    
    for episode in range(num_episodes):
        state = env.reset()
        total_reward = 0
        
        for step in range(steps_per_episode):
            # SÃ©lectionner une action
            action = agent.select_action(state)
            
            # ExÃ©cuter l'action
            next_state, reward, done = env.step(state, action)
            
            # Mettre Ã  jour la Q-table
            agent.update_q_value(state, action, reward, next_state)
            
            # Accumuler la rÃ©compense
            total_reward += reward
            
            # Transition
            state = next_state
            
            if done:
                break
        
        # Decay epsilon
        agent.decay_epsilon()
        
        # Enregistrer la rÃ©compense
        agent.episode_rewards.append(total_reward)
        
        # Afficher progression
        if (episode + 1) % 50 == 0:
            avg_reward = np.mean(agent.episode_rewards[-50:])
            print(f"Episode {episode+1}/{num_episodes} | "
                  f"Avg Reward: {avg_reward:.2f} | "
                  f"Epsilon: {agent.epsilon:.3f}")
    
    print("\nâœ“ EntraÃ®nement terminÃ© !")
    return agent


# ============================================
# PARTIE 4 : VISUALISATION DES RÃ‰SULTATS
# ============================================

def plot_training_results(agent, window=30):
    """
    Visualise la progression de l'entraÃ®nement
    """
    plt.figure(figsize=(14, 5))
    
    # Subplot 1: RÃ©compenses par Ã©pisode
    plt.subplot(1, 2, 1)
    plt.plot(agent.episode_rewards, alpha=0.6, label='Episode Reward')
    
    # Moyenne mobile
    if len(agent.episode_rewards) >= window:
        moving_avg = np.convolve(agent.episode_rewards, 
                                np.ones(window)/window, mode='valid')
        plt.plot(range(window-1, len(agent.episode_rewards)), 
                moving_avg, 'r-', linewidth=2, label=f'Moving Avg ({window})')
    
    plt.xlabel('Episode', fontsize=12)
    plt.ylabel('Cumulative Reward', fontsize=12)
    plt.title('Training Progress', fontsize=14, fontweight='bold')
    plt.legend()
    plt.grid(alpha=0.3)
    
    # Subplot 2: Q-table heatmap
    plt.subplot(1, 2, 2)
    sns.heatmap(agent.Q, annot=True, fmt='.2f', cmap='RdYlGn', 
                xticklabels=['Normal', 'DoS', 'Probe', 'R2L', 'U2R'],
                yticklabels=['Allow', 'Report', 'Return', 'Block'],
                cbar_kws={'label': 'Q-Value'})
    plt.title('Final Q-Table', fontsize=14, fontweight='bold')
    plt.xlabel('States', fontsize=12)
    plt.ylabel('Actions', fontsize=12)
    
    plt.tight_layout()
    plt.savefig('training_results.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    print("âœ“ Graphique sauvegardÃ©: training_results.png")


def display_q_table(agent, env):
    """
    Affiche la Q-table sous forme de DataFrame
    """
    df = pd.DataFrame(
        agent.Q,
        columns=env.states,
        index=env.actions
    )
    print("\n" + "="*60)
    print("Q-TABLE FINALE")
    print("="*60)
    print(df.to_string())
    print()


def display_optimal_policy(policy):
    """
    Affiche la politique optimale
    """
    print("\n" + "="*60)
    print("POLITIQUE OPTIMALE")
    print("="*60)
    for state, action in policy.items():
        emoji = "âœ…" if action == "Allow" else "ðŸ›¡ï¸"
        print(f"{emoji} {state:10s} â†’ {action}")
    print()


# ============================================
# PARTIE 5 : TEST SUR NSL-KDD
# ============================================

class NSLKDDTester:
    """
    Classe pour tester l'agent sur le dataset NSL-KDD
    """
    def __init__(self):
        # Mapping des attaques NSL-KDD vers nos 5 Ã©tats
        self.attack_mapping = {
            'normal': 0,
            # DoS attacks
            'back': 1, 'land': 1, 'neptune': 1, 'pod': 1, 'smurf': 1, 
            'teardrop': 1, 'mailbomb': 1, 'apache2': 1, 'processtable': 1, 
            'udpstorm': 1,
            # Probe attacks
            'ipsweep': 2, 'nmap': 2, 'portsweep': 2, 'satan': 2, 'mscan': 2, 
            'saint': 2,
            # R2L attacks
            'ftp_write': 3, 'guess_passwd': 3, 'imap': 3, 'multihop': 3, 
            'phf': 3, 'spy': 3, 'warezclient': 3, 'warezmaster': 3, 
            'sendmail': 3, 'named': 3, 'snmpgetattack': 3, 'snmpguess': 3, 
            'xlock': 3, 'xsnoop': 3, 'worm': 3,
            # U2R attacks
            'buffer_overflow': 4, 'loadmodule': 4, 'perl': 4, 'rootkit': 4, 
            'httptunnel': 4, 'ps': 4, 'sqlattack': 4, 'xterm': 4
        }
    
    def load_nsl_kdd(self, filepath='KDDTest+.txt', sample_size=None):
        """
        Charge et prÃ©traite le dataset NSL-KDD
        
        Note: Si vous n'avez pas le fichier, ce code gÃ©nÃ¨re des donnÃ©es synthÃ©tiques
        """
        try:
            # Essayer de charger le vrai dataset
            column_names = self._get_column_names()
            data = pd.read_csv(filepath, names=column_names)
            print(f"âœ“ Dataset NSL-KDD chargÃ©: {len(data)} Ã©chantillons")
            
        except FileNotFoundError:
            # GÃ©nÃ©rer des donnÃ©es synthÃ©tiques
            print("âš ï¸  Fichier NSL-KDD non trouvÃ©. GÃ©nÃ©ration de donnÃ©es synthÃ©tiques...")
            data = self._generate_synthetic_data(sample_size or 5000)
        
        # Ã‰chantillonner si demandÃ©
        if sample_size and len(data) > sample_size:
            data = data.sample(n=sample_size, random_state=42)
            print(f"âœ“ Ã‰chantillonnage: {len(data)} Ã©chantillons")
        
        return data
    
    def _get_column_names(self):
        """Retourne les noms de colonnes du dataset NSL-KDD"""
        return ['duration', 'protocol_type', 'service', 'flag', 'src_bytes', 
                'dst_bytes', 'land', 'wrong_fragment', 'urgent', 'hot', 
                'num_failed_logins', 'logged_in', 'num_compromised', 
                'root_shell', 'su_attempted', 'num_root', 'num_file_creations', 
                'num_shells', 'num_access_files', 'num_outbound_cmds', 
                'is_host_login', 'is_guest_login', 'count', 'srv_count', 
                'serror_rate', 'srv_serror_rate', 'rerror_rate', 
                'srv_rerror_rate', 'same_srv_rate', 'diff_srv_rate', 
                'srv_diff_host_rate', 'dst_host_count', 'dst_host_srv_count', 
                'dst_host_same_srv_rate', 'dst_host_diff_srv_rate', 
                'dst_host_same_src_port_rate', 'dst_host_srv_diff_host_rate', 
                'dst_host_serror_rate', 'dst_host_srv_serror_rate', 
                'dst_host_rerror_rate', 'dst_host_srv_rerror_rate', 
                'attack', 'difficulty']
    
    def _generate_synthetic_data(self, n_samples=5000):
        """
        GÃ©nÃ¨re des donnÃ©es synthÃ©tiques simulant NSL-KDD
        """
        np.random.seed(42)
        
        # Distribution des attaques (similaire Ã  NSL-KDD)
        attack_types = ['normal'] * 2500 + \
                      ['neptune', 'smurf', 'back'] * 800 + \
                      ['portsweep', 'ipsweep'] * 300 + \
                      ['guess_passwd', 'warezmaster'] * 150 + \
                      ['buffer_overflow', 'rootkit'] * 50
        
        random.shuffle(attack_types)
        attack_types = attack_types[:n_samples]
        
        # Features numÃ©riques simulÃ©es
        data = {
            'duration': np.random.exponential(2, n_samples),
            'src_bytes': np.random.exponential(500, n_samples),
            'dst_bytes': np.random.exponential(300, n_samples),
            'count': np.random.poisson(10, n_samples),
            'srv_count': np.random.poisson(8, n_samples),
            'serror_rate': np.random.uniform(0, 1, n_samples),
            'srv_serror_rate': np.random.uniform(0, 1, n_samples),
            'rerror_rate': np.random.uniform(0, 1, n_samples),
            'same_srv_rate': np.random.uniform(0, 1, n_samples),
            'diff_srv_rate': np.random.uniform(0, 1, n_samples),
            'attack': attack_types
        }
        
        df = pd.DataFrame(data)
        print(f"âœ“ DonnÃ©es synthÃ©tiques gÃ©nÃ©rÃ©es: {len(df)} Ã©chantillons")
        
        return df
    
    def preprocess_data(self, data):
        """
        PrÃ©traitement: sÃ©lection de features et mapping des Ã©tats
        """
        # SÃ©lectionner les features numÃ©riques importantes
        numeric_features = ['duration', 'src_bytes', 'dst_bytes', 'count', 
                           'srv_count', 'serror_rate', 'srv_serror_rate', 
                           'rerror_rate', 'same_srv_rate', 'diff_srv_rate']
        
        # Garder seulement les features disponibles
        available_features = [f for f in numeric_features if f in data.columns]
        X = data[available_features].fillna(0)
        
        # Normalisation
        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(X)
        
        # Mapper les attaques vers les 5 Ã©tats
        y = data['attack'].apply(lambda x: self.attack_mapping.get(x, 0))
        
        return X_scaled, y
    
    def test_agent(self, agent, X, y, env):
        """
        Teste l'agent sur les donnÃ©es NSL-KDD
        """
        print("\n" + "="*60)
        print("TEST SUR NSL-KDD")
        print("="*60)
        
        cumulative_reward = 0
        actions_taken = []
        expected_actions = []
        
        for i in range(len(y)):
            state = y.iloc[i]
            
            # L'agent choisit la meilleure action
            action = np.argmax(agent.Q[:, state])
            actions_taken.append(action)
            
            # Action attendue (ground truth)
            if state == 0:  # Normal
                expected_actions.append(0)  # Allow
            else:  # Attaque
                expected_actions.append(3)  # Block
            
            # RÃ©compense
            reward = env.rewards[state, action]
            cumulative_reward += reward
        
        # Calcul des mÃ©triques
        accuracy = accuracy_score(expected_actions, actions_taken)
        
        print(f"\nðŸ“Š RÃ‰SULTATS:")
        print(f"  â€¢ Ã‰chantillons testÃ©s: {len(y)}")
        print(f"  â€¢ RÃ©compense cumulative: {cumulative_reward:.2f}")
        print(f"  â€¢ RÃ©compense moyenne: {cumulative_reward/len(y):.2f}")
        print(f"  â€¢ Accuracy: {accuracy:.2%}")
        
        # Distribution des Ã©tats
        state_counts = y.value_counts().sort_index()
        print(f"\nðŸ“ˆ DISTRIBUTION DES Ã‰TATS:")
        state_names = ['Normal', 'DoS', 'Probe', 'R2L', 'U2R']
        for idx, count in state_counts.items():
            print(f"  â€¢ {state_names[idx]:10s}: {count:5d} ({count/len(y)*100:.1f}%)")
        
        # Matrice de confusion
        self._plot_confusion_matrix(expected_actions, actions_taken)
        
        return {
            'cumulative_reward': cumulative_reward,
            'accuracy': accuracy,
            'actions_taken': actions_taken,
            'expected_actions': expected_actions
        }
    
    def _plot_confusion_matrix(self, y_true, y_pred):
        """
        Affiche la matrice de confusion
        """
        cm = confusion_matrix(y_true, y_pred)
        
        plt.figure(figsize=(8, 6))
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                   xticklabels=['Allow', 'Report', 'Return', 'Block'],
                   yticklabels=['Allow', 'Report', 'Return', 'Block'])
        plt.title('Confusion Matrix: Expected vs Actual Actions', 
                 fontsize=14, fontweight='bold')
        plt.xlabel('Predicted Action', fontsize=12)
        plt.ylabel('Expected Action', fontsize=12)
        plt.tight_layout()
        plt.savefig('confusion_matrix.png', dpi=300, bbox_inches='tight')
        plt.show()
        
        print("\nâœ“ Matrice de confusion sauvegardÃ©e: confusion_matrix.png")


# ============================================
# PARTIE 6 : FONCTION PRINCIPALE
# ============================================

def main():
    """
    Fonction principale pour exÃ©cuter tout le pipeline
    """
    print("\n" + "="*60)
    print("AGENT Q-LEARNING POUR DÃ‰TECTION DE CYBERATTAQUES")
    print("="*60 + "\n")
    
    # 1. CrÃ©er l'environnement
    print("ðŸ“‹ Ã‰TAPE 1: CrÃ©ation de l'environnement")
    env = CyberSecurityEnvironment()
    print(f"  â€¢ Ã‰tats: {env.states}")
    print(f"  â€¢ Actions: {env.actions}")
    
    # 2. CrÃ©er l'agent
    print("\nðŸ¤– Ã‰TAPE 2: Initialisation de l'agent")
    agent = QLearningAgent(
        n_states=env.n_states,
        n_actions=env.n_actions,
        learning_rate=0.2,
        discount_factor=0.1,
        epsilon=0.9,
        epsilon_decay=0.05,
        epsilon_min=0.01
    )
    print(f"  â€¢ Learning rate (Î±): {agent.alpha}")
    print(f"  â€¢ Discount factor (Î³): {agent.gamma}")
    print(f"  â€¢ Exploration rate (Îµ): {agent.epsilon}")
    
    # 3. EntraÃ®ner l'agent
    print("\nðŸŽ¯ Ã‰TAPE 3: EntraÃ®nement")
    agent = train_agent(env, agent, num_episodes=300, steps_per_episode=50)
    
    # 4. Afficher les rÃ©sultats
    print("\nðŸ“Š Ã‰TAPE 4: Analyse des rÃ©sultats")
    display_q_table(agent, env)
    
    policy = agent.get_optimal_policy(env.states)
    display_optimal_policy(policy)
    
    avg_reward = np.mean(agent.episode_rewards[-50:])
    print(f"ðŸ’° RÃ©compense moyenne (50 derniers Ã©pisodes): {avg_reward:.2f}")
    
    # 5. Visualiser
    print("\nðŸ“ˆ Ã‰TAPE 5: Visualisation")
    plot_training_results(agent, window=30)
    
    # 6. Tester sur NSL-KDD
    print("\nðŸ§ª Ã‰TAPE 6: Test sur NSL-KDD")
    tester = NSLKDDTester()
    
    # Charger les donnÃ©es (synthÃ©tiques si fichier absent)
    data = tester.load_nsl_kdd(sample_size=5000)
    
    # PrÃ©traiter
    X, y = tester.preprocess_data(data)
    
    # Tester
    results = tester.test_agent(agent, X, y, env)
    
    print("\n" + "="*60)
    print("âœ… PIPELINE TERMINÃ‰ AVEC SUCCÃˆS")
    print("="*60)
    
    return agent, env, results


# ============================================
# EXÃ‰CUTION
# ============================================

if __name__ == "__main__":
    agent, env, results = main()