# PPO-based Cyber Defense Agent using CybORG üõ°Ô∏è 

Ce projet impl√©mente un agent de d√©fense cyber utilisant **l‚Äôapprentissage par renforcement profond (Deep Reinforcement Learning)**. L‚Äôagent, entra√Æn√© avec l‚Äôalgorithme **Proximal Policy Optimization (PPO)**, apprend √† prot√©ger un r√©seau simul√© contre un attaquant automatis√© dans l‚Äôenvironnement **CybORG**.

<img width="500" height="303" alt="image" src="https://github.com/user-attachments/assets/0900f000-d645-4e7f-b885-a14d9e4d12af" />


## Objectif du projet

L‚Äôobjectif principal est d‚Äô√©tudier la capacit√© d‚Äôun agent intelligent √† :
- prendre des d√©cisions d√©fensives adapt√©es dans un environnement cyber dynamique,
- limiter les d√©g√¢ts caus√©s par un attaquant,
- am√©liorer ses performances au fil de l‚Äôentra√Ænement, sans supervision explicite.

L‚Äôagent d√©fensif (*Blue*) est entra√Æn√© contre un attaquant pr√©programm√© (*Red ‚Äì B_lineAgent*).



## Technologies utilis√©es

- **Python**
- **PyTorch** ‚Äì impl√©mentation du r√©seau Actor-Critic
- **CybORG** ‚Äì environnement de simulation cybers√©curit√©
- **Proximal Policy Optimization (PPO)**
- **NumPy**


## Architecture g√©n√©rale

Le syst√®me repose sur une architecture **Actor-Critic** :

- **Actor** : apprend une politique de d√©fense (choix des actions)
- **Critic** : estime la valeur des √©tats
- **Environnement** : CybORG (sc√©nario `Scenario1b.yaml`)
- **Adversaire** : agent Red (`B_lineAgent`)

L‚Äôagent Blue interagit avec l‚Äôenvironnement, collecte des trajectoires, calcule les avantages (GAE) et met √† jour sa politique via PPO.

## Entra√Ænement

L‚Äôentra√Ænement se fait sur plusieurs √©pisodes, chaque √©pisode correspondant √† une simulation compl√®te de d√©fense du r√©seau.

Principaux param√®tres :
- Nombre d‚Äô√©pisodes : 2000
- Steps maximum par √©pisode : 30
- Actions valides limit√©es pour √©viter des p√©nalit√©s inutiles
- Utilisation de **Generalized Advantage Estimation (GAE)**

Pour lancer l‚Äôentra√Ænement :

```bash
python main.py 
```

Les mod√®les sont automatiquement sauvegard√©s :

best_ppo_model.pth : meilleure performance observ√©e

final_ppo_model.pth : √©tat final du mod√®le
