# PPO-based Cyber Defense Agent using CybORG ğŸ›¡ï¸ 
---

Ce projet implÃ©mente un agent de dÃ©fense cyber utilisant **lâ€™apprentissage par renforcement profond (Deep Reinforcement Learning)**. Lâ€™agent, entraÃ®nÃ© avec lâ€™algorithme **Proximal Policy Optimization (PPO)**, apprend Ã  protÃ©ger un rÃ©seau simulÃ© contre un attaquant automatisÃ© dans lâ€™environnement **CybORG**.

<img width="500" height="303" alt="image" src="https://github.com/user-attachments/assets/0900f000-d645-4e7f-b885-a14d9e4d12af" />


## Objectif du projet
---

Lâ€™objectif principal est dâ€™Ã©tudier la capacitÃ© dâ€™un agent intelligent Ã  :
- prendre des dÃ©cisions dÃ©fensives adaptÃ©es dans un environnement cyber dynamique,
- limiter les dÃ©gÃ¢ts causÃ©s par un attaquant,
- amÃ©liorer ses performances au fil de lâ€™entraÃ®nement, sans supervision explicite.

Lâ€™agent dÃ©fensif (*Blue*) est entraÃ®nÃ© contre un attaquant prÃ©programmÃ© (*Red â€“ B_lineAgent*).



## ğŸ§  Technologies utilisÃ©es

- **Python**
- **PyTorch** â€“ implÃ©mentation du rÃ©seau Actor-Critic
- **CybORG** â€“ environnement de simulation cybersÃ©curitÃ©
- **Proximal Policy Optimization (PPO)**
- **NumPy**

---

## âš™ï¸ Architecture gÃ©nÃ©rale

Le systÃ¨me repose sur une architecture **Actor-Critic** :

- **Actor** : apprend une politique de dÃ©fense (choix des actions)
- **Critic** : estime la valeur des Ã©tats
- **Environnement** : CybORG (scÃ©nario `Scenario1b.yaml`)
- **Adversaire** : agent Red (`B_lineAgent`)

Lâ€™agent Blue interagit avec lâ€™environnement, collecte des trajectoires, calcule les avantages (GAE) et met Ã  jour sa politique via PPO.

---

## ğŸ“‚ Structure du projet

â”œâ”€â”€ main.py # Script principal (entraÃ®nement + Ã©valuation)
â”œâ”€â”€ best_ppo_model.pth # Meilleur modÃ¨le sauvegardÃ©
â”œâ”€â”€ final_ppo_model.pth # ModÃ¨le final aprÃ¨s entraÃ®nement
â”œâ”€â”€ README.md # Documentation du projet


---

## ğŸš€ EntraÃ®nement

Lâ€™entraÃ®nement se fait sur plusieurs Ã©pisodes, chaque Ã©pisode correspondant Ã  une simulation complÃ¨te de dÃ©fense du rÃ©seau.

Principaux paramÃ¨tres :
- Nombre dâ€™Ã©pisodes : 2000
- Steps maximum par Ã©pisode : 30
- Actions valides limitÃ©es pour Ã©viter des pÃ©nalitÃ©s inutiles
- Utilisation de **Generalized Advantage Estimation (GAE)**

Pour lancer lâ€™entraÃ®nement :

```bash
python main.py

Les modÃ¨les sont automatiquement sauvegardÃ©s :

best_ppo_model.pth : meilleure performance observÃ©e

final_ppo_model.pth : Ã©tat final du modÃ¨le
